{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "- https://www.kaggle.com/code/ahmadbajwa9898/ecg-classification\n",
    "- https://www.kaggle.com/code/mo7amedsabry/ecg-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_path = \"dataset/ptbdb_normal.csv\"\n",
    "abnormal_path = \"dataset/ptbdb_abnormal.csv\"\n",
    "def load_data():\n",
    "    normal = pd.read_csv(normal_path, header=None)\n",
    "    abnormal = pd.read_csv(abnormal_path, header=None)\n",
    "    \n",
    "    return normal, abnormal\n",
    "\n",
    "normal, abnormal = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "def plot_sample_ecgs(normal, abnormal, num_samples=3):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for i in range(num_samples):\n",
    "        plt.plot(normal.iloc[i, :-1], label=f'Sample {i+1}')\n",
    "    plt.title('Normal ECG Samples')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i in range(num_samples):\n",
    "        plt.plot(abnormal.iloc[i, :-1], label=f'Sample {i+1}')\n",
    "    plt.title('Abnormal ECG Samples')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_ecgs(normal, abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Normal - value counts: {normal.iloc[:,-1].value_counts()} \\n\\nAbnormal - value counts: {abnormal.iloc[:,-1].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values are about half for abnormal, we need to apply either drop columns from normal or apply oversampling to abnormal (or a little bit of both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_to_drop = 0.4  # Drop 20% of rows\n",
    "abnormal_reduced = abnormal.drop(abnormal.sample(frac=frac_to_drop, random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abnormal_reduced.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal = abnormal_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_balance_data(normal_path, abnormal_path, train_size=0.8, val_test_split=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Load and balance ECG data with train/val/test splits (80/10/10)\n",
    "    \n",
    "    Args:\n",
    "        normal_path: Path to normal ECG data\n",
    "        abnormal_path: Path to abnormal ECG data\n",
    "        train_size: Ratio of data for training (default 0.8)\n",
    "        val_test_split: Ratio of remaining data to allocate to validation (default 0.5)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n",
    "    \"\"\"\n",
    "    normal['label'] = 0\n",
    "    abnormal['label'] = 1\n",
    "    \n",
    "    data = pd.concat([normal, abnormal], axis=0)\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    class_counts = np.bincount(y)\n",
    "    total_samples = len(y)\n",
    "    class_weights = {\n",
    "        0: total_samples / (2 * class_counts[0]),  # Normal\n",
    "        1: total_samples / (2 * class_counts[1])   # Abnormal\n",
    "    }\n",
    "    \n",
    "    # First split: train vs (val + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        train_size=train_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test from remaining data\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_test_split,  \n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Train samples: {len(X_train)} ({len(X_train)/len(X):.1%})\")\n",
    "    print(f\"Validation samples: {len(X_val)} ({len(X_val)/len(X):.1%})\")\n",
    "    print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X):.1%})\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_and_balance_data(normal_path=normal, abnormal_path=abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y_train, y_val, y_test):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    sns.countplot(x=y_train, ax=ax[0])\n",
    "    ax[0].set_title(f'Train Set\\n({len(y_train)} samples)')\n",
    "    ax[0].set_xticklabels(['Normal', 'Abnormal'])\n",
    "    \n",
    "    sns.countplot(x=y_val, ax=ax[1])\n",
    "    ax[1].set_title(f'Validation Set\\n({len(y_val)} samples)')\n",
    "    ax[1].set_xticklabels(['Normal', 'Abnormal'])\n",
    "    \n",
    "    sns.countplot(x=y_test, ax=ax[2])\n",
    "    ax[2].set_title(f'Test Set\\n({len(y_test)} samples)')\n",
    "    ax[2].set_xticklabels(['Normal', 'Abnormal'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is way less class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        # First Conv Block\n",
    "        layers.Conv1D(64, kernel_size=15, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Second Conv Block\n",
    "        layers.Conv1D(128, kernel_size=11, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "                # Third Conv Block\n",
    "        layers.Conv1D(256, kernel_size=7, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        \n",
    "        layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_auc',  \n",
    "    patience=15,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max')\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,  \n",
    "    batch_size=128,\n",
    "    class_weight=class_weights,  \n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model, X_test, y_test):\n",
    "    # 1. Model Evaluation\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "    print(f\"Test AUC: {test_results[2]:.4f}\")\n",
    "    \n",
    "    # 2. Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # 3. Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes, \n",
    "                              target_names=['Normal', 'Abnormal']))\n",
    "    \n",
    "    # 4. Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Abnormal'],\n",
    "                yticklabels=['Normal', 'Abnormal'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(6):\n",
    "        idx = np.random.randint(0, len(X_test))\n",
    "        ecg = X_test[idx].flatten()\n",
    "        true_label = y_test[idx]\n",
    "        pred_prob = y_pred[idx][0]\n",
    "        \n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.plot(ecg)\n",
    "        plt.title(f\"True: {'Abnormal' if true_label else 'Normal'}\\n\"\n",
    "                 f\"Predicted: {'Abnormal' if pred_prob > 0.5 else 'Normal'} \"\n",
    "                 f\"({pred_prob:.2f})\")\n",
    "        plt.xlabel('Time steps')\n",
    "        plt.ylabel('Amplitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_and_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ecg_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model.keras') \n",
    "model.summary()\n",
    "sample_input = X_test[0:1]  \n",
    "prediction = model.predict(sample_input)\n",
    "print(f\"Prediction: {'Abnormal' if prediction > 0.5 else 'Normal'} ({prediction[0][0]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_plot(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
